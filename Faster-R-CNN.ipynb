{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10709156,"sourceType":"datasetVersion","datasetId":6637202}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torchvision\nimport os\nfrom torch.utils.data import DataLoader\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.datasets import CocoDetection\nfrom torchvision.transforms import functional as F\nimport matplotlib.pyplot as plt\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:34:44.808173Z","iopub.execute_input":"2025-02-12T04:34:44.808451Z","iopub.status.idle":"2025-02-12T04:34:50.321944Z","shell.execute_reply.started":"2025-02-12T04:34:44.808423Z","shell.execute_reply":"2025-02-12T04:34:50.321307Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Set dataset paths\nTRAIN_IMG_DIR = \"/kaggle/input/fasterrcnntealeaf/dataset/train\"\nTRAIN_ANN_FILE = \"/kaggle/input/fasterrcnntealeaf/dataset/train/coco_annotations_train.json\"\n\nVAL_IMG_DIR = \"/kaggle/input/fasterrcnntealeaf/dataset/val\"\nVAL_ANN_FILE = \"/kaggle/input/fasterrcnntealeaf/dataset/val/coco_annotations_val.json\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:34:58.121972Z","iopub.execute_input":"2025-02-12T04:34:58.122324Z","iopub.status.idle":"2025-02-12T04:34:58.126066Z","shell.execute_reply.started":"2025-02-12T04:34:58.122299Z","shell.execute_reply":"2025-02-12T04:34:58.125168Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Define transformations\nclass CocoTransform:\n    def __call__(self, image, target):\n        image = F.to_tensor(image)  \n        return image, target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:35:00.790452Z","iopub.execute_input":"2025-02-12T04:35:00.790775Z","iopub.status.idle":"2025-02-12T04:35:00.794611Z","shell.execute_reply.started":"2025-02-12T04:35:00.790747Z","shell.execute_reply":"2025-02-12T04:35:00.793805Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Dataset class\ndef get_coco_dataset(img_dir, ann_file):\n    return CocoDetection(\n        root=img_dir,\n        annFile=ann_file,\n        transform=lambda img: F.to_tensor(img)  # Apply transformation correctly\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:35:03.573867Z","iopub.execute_input":"2025-02-12T04:35:03.574180Z","iopub.status.idle":"2025-02-12T04:35:03.578154Z","shell.execute_reply.started":"2025-02-12T04:35:03.574157Z","shell.execute_reply":"2025-02-12T04:35:03.577229Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Load datasets\ntrain_dataset = get_coco_dataset(TRAIN_IMG_DIR, TRAIN_ANN_FILE)\nval_dataset = get_coco_dataset(VAL_IMG_DIR, VAL_ANN_FILE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:35:07.383510Z","iopub.execute_input":"2025-02-12T04:35:07.383808Z","iopub.status.idle":"2025-02-12T04:35:07.436097Z","shell.execute_reply.started":"2025-02-12T04:35:07.383787Z","shell.execute_reply":"2025-02-12T04:35:07.435271Z"}},"outputs":[{"name":"stdout","text":"loading annotations into memory...\nDone (t=0.03s)\ncreating index...\nindex created!\nloading annotations into memory...\nDone (t=0.00s)\ncreating index...\nindex created!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# DataLoader\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:35:10.332657Z","iopub.execute_input":"2025-02-12T04:35:10.332968Z","iopub.status.idle":"2025-02-12T04:35:10.337499Z","shell.execute_reply.started":"2025-02-12T04:35:10.332944Z","shell.execute_reply":"2025-02-12T04:35:10.336513Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Load Faster R-CNN with ResNet-50 backbone\ndef get_model(num_classes):\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n    \n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    \n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:35:13.524348Z","iopub.execute_input":"2025-02-12T04:35:13.524646Z","iopub.status.idle":"2025-02-12T04:35:13.528840Z","shell.execute_reply.started":"2025-02-12T04:35:13.524623Z","shell.execute_reply":"2025-02-12T04:35:13.527964Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Initialize the model\nnum_classes = 9  # Background + objects\nmodel = get_model(num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:35:16.919483Z","iopub.execute_input":"2025-02-12T04:35:16.919770Z","iopub.status.idle":"2025-02-12T04:35:18.550810Z","shell.execute_reply.started":"2025-02-12T04:35:16.919750Z","shell.execute_reply":"2025-02-12T04:35:18.550107Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n100%|██████████| 160M/160M [00:00<00:00, 215MB/s] \n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Move model to GPU if available\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nmodel.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:35:20.726285Z","iopub.execute_input":"2025-02-12T04:35:20.726581Z","iopub.status.idle":"2025-02-12T04:35:21.032582Z","shell.execute_reply.started":"2025-02-12T04:35:20.726558Z","shell.execute_reply":"2025-02-12T04:35:21.031812Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"FasterRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPN(\n    (body): IntermediateLayerGetter(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): FrozenBatchNorm2d(256, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(512, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(1024, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(2048, eps=0.0)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n    (fpn): FeaturePyramidNetwork(\n      (inner_blocks): ModuleList(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (layer_blocks): ModuleList(\n        (0-3): 4 x Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n      (extra_blocks): LastLevelMaxPool()\n    )\n  )\n  (rpn): RegionProposalNetwork(\n    (anchor_generator): AnchorGenerator()\n    (head): RPNHead(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): ReLU(inplace=True)\n        )\n      )\n      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): RoIHeads(\n    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n    (box_head): TwoMLPHead(\n      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n    )\n    (box_predictor): FastRCNNPredictor(\n      (cls_score): Linear(in_features=1024, out_features=9, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=36, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Define optimizer and scheduler\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:35:28.061716Z","iopub.execute_input":"2025-02-12T04:35:28.062094Z","iopub.status.idle":"2025-02-12T04:35:28.067284Z","shell.execute_reply.started":"2025-02-12T04:35:28.062015Z","shell.execute_reply":"2025-02-12T04:35:28.066359Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Training function\ndef train_one_epoch(model, optimizer, data_loader, device, epoch):\n    model.train()\n    for images, targets in data_loader:\n        images = [img.to(device) for img in images]\n\n        processed_targets = []\n        valid_images = []\n        for i in range(len(targets)):\n            target = targets[i]  \n            boxes = []\n            labels = []\n\n            for obj in target:\n                bbox = obj[\"bbox\"]  \n                x, y, w, h = bbox\n                if w > 0 and h > 0:\n                    boxes.append([x, y, x + w, y + h])\n                    labels.append(obj[\"category_id\"])\n\n            if boxes:\n                processed_target = {\n                    \"boxes\": torch.tensor(boxes, dtype=torch.float32).to(device),\n                    \"labels\": torch.tensor(labels, dtype=torch.int64).to(device),\n                }\n                processed_targets.append(processed_target)\n                valid_images.append(images[i])  \n\n        if not processed_targets:\n            continue  \n\n        images = valid_images\n        loss_dict = model(images, processed_targets)\n        losses = sum(loss for loss in loss_dict.values())\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n    print(f\"Epoch [{epoch}] Loss: {losses.item():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:36:34.917009Z","iopub.execute_input":"2025-02-12T04:36:34.917335Z","iopub.status.idle":"2025-02-12T04:36:34.923882Z","shell.execute_reply.started":"2025-02-12T04:36:34.917310Z","shell.execute_reply":"2025-02-12T04:36:34.923064Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Training loop\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    train_one_epoch(model, optimizer, train_loader, device, epoch)\n    lr_scheduler.step()\n    \n    model_path = f\"/kaggle/working/fasterrcnn_resnet50_epoch_{epoch + 1}.pth\"\n    torch.save(model.state_dict(), model_path)\n    print(f\"Model saved: {model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T04:50:00.289370Z","iopub.execute_input":"2025-02-12T04:50:00.289709Z","iopub.status.idle":"2025-02-12T09:09:56.925383Z","shell.execute_reply.started":"2025-02-12T04:50:00.289671Z","shell.execute_reply":"2025-02-12T09:09:56.923792Z"}},"outputs":[{"name":"stdout","text":"Epoch [0] Loss: 0.1225\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_1.pth\nEpoch [1] Loss: 0.0974\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_2.pth\nEpoch [2] Loss: 0.0451\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_3.pth\nEpoch [3] Loss: 0.0226\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_4.pth\nEpoch [4] Loss: 0.0663\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_5.pth\nEpoch [5] Loss: 0.0227\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_6.pth\nEpoch [6] Loss: 0.0367\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_7.pth\nEpoch [7] Loss: 0.0209\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_8.pth\nEpoch [8] Loss: 0.0656\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_9.pth\nEpoch [9] Loss: 0.0464\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_10.pth\nEpoch [10] Loss: 0.0157\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_11.pth\nEpoch [11] Loss: 0.0309\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_12.pth\nEpoch [12] Loss: 0.0790\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_13.pth\nEpoch [13] Loss: 0.0215\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_14.pth\nEpoch [14] Loss: 0.0369\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_15.pth\nEpoch [15] Loss: 0.0422\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_16.pth\nEpoch [16] Loss: 0.0389\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_17.pth\nEpoch [17] Loss: 0.0298\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_18.pth\nEpoch [18] Loss: 0.0286\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_19.pth\nEpoch [19] Loss: 0.0301\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_20.pth\nEpoch [20] Loss: 0.0505\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_21.pth\nEpoch [21] Loss: 0.0194\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_22.pth\nEpoch [22] Loss: 0.0327\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_23.pth\nEpoch [23] Loss: 0.0139\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_24.pth\nEpoch [24] Loss: 0.1320\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_25.pth\nEpoch [25] Loss: 0.0451\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_26.pth\nEpoch [26] Loss: 0.0162\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_27.pth\nEpoch [27] Loss: 0.0230\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_28.pth\nEpoch [28] Loss: 0.0315\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_29.pth\nEpoch [29] Loss: 0.0489\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_30.pth\nEpoch [30] Loss: 0.0404\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_31.pth\nEpoch [31] Loss: 0.0229\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_32.pth\nEpoch [32] Loss: 0.0397\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_33.pth\nEpoch [33] Loss: 0.0419\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_34.pth\nEpoch [34] Loss: 0.0948\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_35.pth\nEpoch [35] Loss: 0.0722\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_36.pth\nEpoch [36] Loss: 0.0725\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_37.pth\nEpoch [37] Loss: 0.0205\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_38.pth\nEpoch [38] Loss: 0.0322\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_39.pth\nEpoch [39] Loss: 0.0671\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_40.pth\nEpoch [40] Loss: 0.0465\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_41.pth\nEpoch [41] Loss: 0.0129\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_42.pth\nEpoch [42] Loss: 0.0813\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_43.pth\nEpoch [43] Loss: 0.0437\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_44.pth\nEpoch [44] Loss: 0.0299\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_45.pth\nEpoch [45] Loss: 0.0784\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_46.pth\nEpoch [46] Loss: 0.0920\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_47.pth\nEpoch [47] Loss: 0.0410\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_48.pth\nEpoch [48] Loss: 0.0075\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_49.pth\nEpoch [49] Loss: 0.1209\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_50.pth\nEpoch [50] Loss: 0.0206\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_51.pth\nEpoch [51] Loss: 0.0843\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_52.pth\nEpoch [52] Loss: 0.0204\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_53.pth\nEpoch [53] Loss: 0.0244\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_54.pth\nEpoch [54] Loss: 0.0144\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_55.pth\nEpoch [55] Loss: 0.0320\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_56.pth\nEpoch [56] Loss: 0.0397\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_57.pth\nEpoch [57] Loss: 0.1096\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_58.pth\nEpoch [58] Loss: 0.0190\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_59.pth\nEpoch [59] Loss: 0.0260\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_60.pth\nEpoch [60] Loss: 0.0248\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_61.pth\nEpoch [61] Loss: 0.0658\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_62.pth\nEpoch [62] Loss: 0.0541\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_63.pth\nEpoch [63] Loss: 0.1183\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_64.pth\nEpoch [64] Loss: 0.0422\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_65.pth\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-71efd792f172>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-b2a34e014017>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessed_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[operator]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mproposals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_coder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_bbox_deltas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mproposals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m         \u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_proposals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobjectness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_anchors_per_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mfilter_proposals\u001b[0;34m(self, proposals, objectness, image_shapes, num_anchors_per_level)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;31m# non-maximum suppression, independently done per level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0mkeep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbox_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatched_nms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlvl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnms_thresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;31m# keep only topk scoring predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/ops/boxes.py\u001b[0m in \u001b[0;36mbatched_nms\u001b[0;34m(boxes, scores, idxs, iou_threshold)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# https://github.com/pytorch/vision/issues/1311#issuecomment-781329339\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4000\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_batched_nms_vanilla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_batched_nms_coordinate_trick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/jit/_trace.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1442\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[0;31m# Not tracing, don't do anything\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1444\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m         \u001b[0mcompiled_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__original_fn\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/ops/boxes.py\u001b[0m in \u001b[0;36m_batched_nms_vanilla\u001b[0;34m(boxes, scores, idxs, iou_threshold)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mclass_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mcurr_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midxs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mclass_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mcurr_keep_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurr_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurr_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mkeep_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurr_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurr_keep_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mkeep_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/ops/boxes.py\u001b[0m in \u001b[0;36mnms\u001b[0;34m(boxes, scores, iou_threshold)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0m_assert_has_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_torchbind_op_overload\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_must_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_call_overload_packet_from_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"import torch\n\n# Define the path of the last saved model\ncheckpoint_path = \"/kaggle/working/fasterrcnn_resnet50_epoch_63.pth\"\n\n# Load the model weights\nmodel.load_state_dict(torch.load(checkpoint_path))\n\nprint(f\"Loaded model from {checkpoint_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T09:19:47.552555Z","iopub.execute_input":"2025-02-12T09:19:47.552866Z","iopub.status.idle":"2025-02-12T09:19:47.737455Z","shell.execute_reply.started":"2025-02-12T09:19:47.552839Z","shell.execute_reply":"2025-02-12T09:19:47.736612Z"}},"outputs":[{"name":"stdout","text":"Loaded model from /kaggle/working/fasterrcnn_resnet50_epoch_63.pth\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-30-b8e51afaaffa>:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(checkpoint_path))\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"num_epochs = 100  # Your total target epochs\nstart_epoch = 63  # Resume from the last saved epoch\n\nfor epoch in range(start_epoch, num_epochs):  # Start from epoch 63\n    train_one_epoch(model, optimizer, train_loader, device, epoch)\n    lr_scheduler.step()\n\n    # Save model after each epoch\n    model_path = f\"/kaggle/working/fasterrcnn_resnet50_epoch_{epoch + 1}.pth\"\n    torch.save(model.state_dict(), model_path)\n    print(f\"Model saved: {model_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T09:20:03.557669Z","iopub.execute_input":"2025-02-12T09:20:03.557982Z","iopub.status.idle":"2025-02-12T11:46:14.809351Z","shell.execute_reply.started":"2025-02-12T09:20:03.557957Z","shell.execute_reply":"2025-02-12T11:46:14.808524Z"}},"outputs":[{"name":"stdout","text":"Epoch [63] Loss: 0.0221\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_64.pth\nEpoch [64] Loss: 0.0113\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_65.pth\nEpoch [65] Loss: 0.0833\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_66.pth\nEpoch [66] Loss: 0.0433\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_67.pth\nEpoch [67] Loss: 0.0117\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_68.pth\nEpoch [68] Loss: 0.0545\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_69.pth\nEpoch [69] Loss: 0.1406\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_70.pth\nEpoch [70] Loss: 0.2485\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_71.pth\nEpoch [71] Loss: 0.0114\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_72.pth\nEpoch [72] Loss: 0.0112\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_73.pth\nEpoch [73] Loss: 0.0176\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_74.pth\nEpoch [74] Loss: 0.0193\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_75.pth\nEpoch [75] Loss: 0.0159\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_76.pth\nEpoch [76] Loss: 0.0211\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_77.pth\nEpoch [77] Loss: 0.0637\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_78.pth\nEpoch [78] Loss: 0.0975\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_79.pth\nEpoch [79] Loss: 0.0461\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_80.pth\nEpoch [80] Loss: 0.0509\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_81.pth\nEpoch [81] Loss: 0.0322\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_82.pth\nEpoch [82] Loss: 0.0271\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_83.pth\nEpoch [83] Loss: 0.0263\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_84.pth\nEpoch [84] Loss: 0.0174\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_85.pth\nEpoch [85] Loss: 0.0277\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_86.pth\nEpoch [86] Loss: 0.0892\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_87.pth\nEpoch [87] Loss: 0.1143\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_88.pth\nEpoch [88] Loss: 0.0810\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_89.pth\nEpoch [89] Loss: 0.0139\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_90.pth\nEpoch [90] Loss: 0.0432\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_91.pth\nEpoch [91] Loss: 0.0242\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_92.pth\nEpoch [92] Loss: 0.0123\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_93.pth\nEpoch [93] Loss: 0.0293\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_94.pth\nEpoch [94] Loss: 0.0202\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_95.pth\nEpoch [95] Loss: 0.0536\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_96.pth\nEpoch [96] Loss: 0.0360\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_97.pth\nEpoch [97] Loss: 0.0108\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_98.pth\nEpoch [98] Loss: 0.0253\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_99.pth\nEpoch [99] Loss: 0.0178\nModel saved: /kaggle/working/fasterrcnn_resnet50_epoch_100.pth\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import torch\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\n\n# Define model and checkpoint path\ncheckpoint_path = \"/kaggle/working/fasterrcnn_resnet50_epoch_100.pth\"\n\n# Load the model architecture\nmodel = fasterrcnn_resnet50_fpn(pretrained=False, num_classes=9)  # Set NUM_CLASSES to match your training\n\n# Load trained weights\nmodel.load_state_dict(torch.load(checkpoint_path, map_location=torch.device(\"cpu\")))\n\n# Set model to evaluation mode\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T14:09:08.453643Z","iopub.execute_input":"2025-02-12T14:09:08.454009Z","iopub.status.idle":"2025-02-12T14:09:09.199486Z","shell.execute_reply.started":"2025-02-12T14:09:08.453978Z","shell.execute_reply":"2025-02-12T14:09:09.198575Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-67-d16e7489085a>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(checkpoint_path, map_location=torch.device(\"cpu\")))\n","output_type":"stream"},{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"FasterRCNN(\n  (transform): GeneralizedRCNNTransform(\n      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n  )\n  (backbone): BackboneWithFPN(\n    (body): IntermediateLayerGetter(\n      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n      (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n      (relu): ReLU(inplace=True)\n      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n      (layer1): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): FrozenBatchNorm2d(256, eps=1e-05)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(64, eps=1e-05)\n          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(64, eps=1e-05)\n          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(256, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer2): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(512, eps=1e-05)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(128, eps=1e-05)\n          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(512, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer3): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(1024, eps=1e-05)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (3): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (4): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (5): Bottleneck(\n          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(256, eps=1e-05)\n          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(1024, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n      )\n      (layer4): Sequential(\n        (0): Bottleneck(\n          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n          (relu): ReLU(inplace=True)\n          (downsample): Sequential(\n            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n            (1): FrozenBatchNorm2d(2048, eps=1e-05)\n          )\n        )\n        (1): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n        (2): Bottleneck(\n          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn1): FrozenBatchNorm2d(512, eps=1e-05)\n          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (bn2): FrozenBatchNorm2d(512, eps=1e-05)\n          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn3): FrozenBatchNorm2d(2048, eps=1e-05)\n          (relu): ReLU(inplace=True)\n        )\n      )\n    )\n    (fpn): FeaturePyramidNetwork(\n      (inner_blocks): ModuleList(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n        )\n      )\n      (layer_blocks): ModuleList(\n        (0-3): 4 x Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        )\n      )\n      (extra_blocks): LastLevelMaxPool()\n    )\n  )\n  (rpn): RegionProposalNetwork(\n    (anchor_generator): AnchorGenerator()\n    (head): RPNHead(\n      (conv): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): ReLU(inplace=True)\n        )\n      )\n      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n    )\n  )\n  (roi_heads): RoIHeads(\n    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n    (box_head): TwoMLPHead(\n      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n    )\n    (box_predictor): FastRCNNPredictor(\n      (cls_score): Linear(in_features=1024, out_features=9, bias=True)\n      (bbox_pred): Linear(in_features=1024, out_features=36, bias=True)\n    )\n  )\n)"},"metadata":{}}],"execution_count":67},{"cell_type":"code","source":"import torchvision.transforms as T\nfrom PIL import Image\nimport os\n\n# Define test images folder\ntest_images_folder = \"/kaggle/input/fasterrcnntealeaf/dataset/test\"\n\n# Define transformation\ntransform = T.Compose([\n    T.ToTensor(),  # Convert image to tensor\n])\n\n# Load test images\ntest_images = [f for f in os.listdir(test_images_folder) if f.endswith(('.jpg', '.png', '.jpeg'))]\n\n# Process and test each image\nfor image_name in test_images:\n    image_path = os.path.join(test_images_folder, image_name)\n    \n    # Load and preprocess the image\n    image = Image.open(image_path).convert(\"RGB\")\n    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n    \n    # Make prediction\n    with torch.no_grad():\n        predictions = model(image_tensor)\n\n    # Extract results\n    boxes = predictions[0]['boxes'].cpu().numpy()  # Bounding boxes\n    scores = predictions[0]['scores'].cpu().numpy()  # Confidence scores\n    labels = predictions[0]['labels'].cpu().numpy()  # Class labels\n\n    # Print results\n    print(f\"\\nResults for {image_name}:\")\n    for i in range(len(scores)):\n        if scores[i] > 0.5:  # Confidence threshold\n            print(f\"Class: {labels[i]}, Score: {scores[i]:.3f}, Box: {boxes[i]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T14:09:18.208631Z","iopub.execute_input":"2025-02-12T14:09:18.208956Z","iopub.status.idle":"2025-02-12T14:17:57.744981Z","shell.execute_reply.started":"2025-02-12T14:09:18.208928Z","shell.execute_reply":"2025-02-12T14:17:57.744058Z"}},"outputs":[{"name":"stdout","text":"\nResults for img1738.jpg:\nClass: 4, Score: 0.988, Box: [297.21744 113.58472 453.2644  351.47906]\nClass: 4, Score: 0.932, Box: [265.05563 389.7437  471.94366 546.9726 ]\n\nResults for img1119.jpg:\nClass: 4, Score: 0.912, Box: [225.1176  128.62128 482.31015 495.16586]\n\nResults for img1743.jpg:\nClass: 4, Score: 0.950, Box: [313.64157 166.77107 422.80002 306.30478]\nClass: 1, Score: 0.862, Box: [184.80573  94.23557 463.89932 560.4279 ]\n\nResults for img2823.jpg:\nClass: 8, Score: 0.988, Box: [133.43404 109.35784 444.61807 544.9323 ]\n\nResults for img2895.jpg:\nClass: 4, Score: 0.519, Box: [296.45593 117.39712 446.46588 255.94115]\n\nResults for img2588.jpg:\nClass: 7, Score: 0.996, Box: [ 12.19126 174.25197 623.4733  479.56332]\n\nResults for img2328.jpg:\nClass: 7, Score: 0.999, Box: [128.45103  52.00593 477.82608 591.3063 ]\n\nResults for img749.jpg:\nClass: 3, Score: 0.979, Box: [112.54307   43.736843 566.2528   597.35614 ]\n\nResults for img752.jpg:\nClass: 3, Score: 0.978, Box: [ 43.08374 117.25968 567.7442  512.2472 ]\n\nResults for img1984.jpg:\nClass: 4, Score: 0.979, Box: [219.13077 264.58566 511.77765 482.1277 ]\n\nResults for WhatsApp Image 2024-10-22 at 15.43.32_bfe9e469.jpg:\nClass: 5, Score: 0.984, Box: [343.20718 621.2147  557.15826 831.0488 ]\n\nResults for img2590.jpg:\nClass: 7, Score: 0.995, Box: [ 37.51304 232.97765 631.0367  496.29126]\n\nResults for img1730.jpg:\nClass: 5, Score: 0.992, Box: [219.15071 421.30475 358.27048 548.5785 ]\nClass: 2, Score: 0.751, Box: [153.73306   17.605055 482.19736  523.4871  ]\n\nResults for img1646.jpg:\nClass: 1, Score: 0.756, Box: [ 78.292946 169.94527  566.6001   510.13965 ]\n\nResults for img921.jpg:\nClass: 3, Score: 0.974, Box: [ 32.389404 111.811584 608.44934  498.42844 ]\n\nResults for img2329.jpg:\nClass: 7, Score: 0.995, Box: [ 18.074488 184.92741  577.69965  492.70566 ]\n\nResults for img591.jpg:\nClass: 8, Score: 0.784, Box: [ 41.55945 115.95861 601.58997 504.13687]\nClass: 1, Score: 0.667, Box: [ 61.940456 133.21976  575.73846  489.38916 ]\n\nResults for img2317.jpg:\nClass: 7, Score: 0.998, Box: [111.461464  42.9635   496.7253   631.2475  ]\n\nResults for img2897.jpg:\nClass: 8, Score: 0.995, Box: [ 64.70625 201.75563 551.2843  553.4672 ]\n\nResults for img119.jpg:\nClass: 1, Score: 0.988, Box: [174.49457  99.3998  462.72876 597.2346 ]\n\nResults for img2587.jpg:\nClass: 7, Score: 0.984, Box: [ 17.122852 168.4738   638.4545   481.38693 ]\n\nResults for img2336.jpg:\nClass: 7, Score: 0.998, Box: [ 69.05694    8.038867 523.8925   613.76056 ]\n\nResults for img2925.jpg:\nClass: 8, Score: 0.995, Box: [ 67.8218  185.3769  548.7631  524.96814]\n\nResults for img1988.jpg:\nClass: 5, Score: 0.959, Box: [402.10043 343.4678  482.88852 427.2128 ]\nClass: 5, Score: 0.919, Box: [169.95268 215.9993  251.6834  285.414  ]\n\nResults for img1732.jpg:\nClass: 5, Score: 0.960, Box: [298.80325 285.65842 424.91357 405.56244]\n\nResults for img2835.jpg:\nClass: 8, Score: 0.998, Box: [ 90.88311 159.93158 561.31024 460.85544]\n\nResults for img1127.jpg:\nClass: 4, Score: 0.943, Box: [294.4321  273.14597 453.81454 439.99713]\n\nResults for img1111.jpg:\nClass: 4, Score: 0.826, Box: [113.902885 297.3523   309.78253  463.96646 ]\n\nResults for img2896.jpg:\nClass: 8, Score: 0.830, Box: [ 67.30718 159.59395 602.88654 494.72818]\n\nResults for img1734.jpg:\nClass: 5, Score: 0.986, Box: [296.603   348.5419  402.33994 452.71622]\n\nResults for img1733.jpg:\nClass: 5, Score: 0.971, Box: [221.10172 238.61015 338.90878 345.15982]\nClass: 5, Score: 0.907, Box: [352.00955 397.66507 445.784   501.51773]\n\nResults for img1635.jpg:\nClass: 7, Score: 0.988, Box: [ 12.552881 178.28989  624.5865   516.5009  ]\n\nResults for img2118.jpg:\nClass: 6, Score: 0.996, Box: [ 83.6426  165.73778 629.07275 456.74658]\n\nResults for img2838.jpg:\nClass: 8, Score: 0.998, Box: [ 21.898146 164.29532  622.06647  510.0416  ]\n\nResults for WhatsApp Image 2024-10-22 at 16.10.30_939018d5.jpg:\nClass: 8, Score: 0.836, Box: [   0.        0.     1123.7247  925.0606]\n\nResults for img924.jpg:\nClass: 3, Score: 0.959, Box: [ 90.53506   13.054443 555.87415  625.48065 ]\n\nResults for img2318.jpg:\nClass: 4, Score: 0.625, Box: [192.98659 183.1317  442.0965  548.5356 ]\nClass: 6, Score: 0.621, Box: [175.85493 129.29031 450.04492 547.9175 ]\n\nResults for img1737.jpg:\nClass: 5, Score: 0.972, Box: [311.8064  189.05371 452.06363 320.45352]\nClass: 2, Score: 0.750, Box: [ 85.768      5.464502 529.8051   620.61865 ]\n\nResults for img1728.jpg:\nClass: 4, Score: 0.977, Box: [247.3046  221.93022 434.1884  535.3479 ]\n\nResults for img2125.jpg:\nClass: 6, Score: 0.853, Box: [ 27.123096 181.87198  640.       487.00836 ]\n\nResults for img2894.jpg:\nClass: 6, Score: 0.927, Box: [ 94.33474 181.90677 544.9085  475.8833 ]\n\nResults for img625.jpg:\nClass: 3, Score: 0.905, Box: [120.86596    4.610376 529.392    634.1398  ]\n\nResults for 1.jpg:\nClass: 5, Score: 0.925, Box: [487.36612 418.0265  735.10394 770.56506]\n\nResults for img751.jpg:\nClass: 3, Score: 0.990, Box: [121.77598   22.829199 550.465    606.88245 ]\n\nResults for img1634.jpg:\nClass: 7, Score: 0.990, Box: [ 37.04182  123.406006 616.6098   554.45233 ]\nClass: 4, Score: 0.986, Box: [ 10.575317 152.52287  182.87482  469.5837  ]\n\nResults for img2045.jpg:\nClass: 6, Score: 0.939, Box: [138.23065 111.41316 407.15454 620.5952 ]\n\nResults for img926.jpg:\nClass: 3, Score: 0.993, Box: [ 99.51507  29.19165 516.1362  615.7067 ]\n\nResults for img2586.jpg:\nClass: 7, Score: 0.768, Box: [ 21.462769 171.158    608.71075  448.1389  ]\n\nResults for img2332.jpg:\nClass: 7, Score: 0.989, Box: [146.71051   48.304836 505.75534  599.40283 ]\n\nResults for img2049.jpg:\nClass: 6, Score: 0.994, Box: [143.12321 187.54793 414.78662 516.1421 ]\n\nResults for img2834.jpg:\nClass: 8, Score: 0.996, Box: [ 55.332447 169.11205  562.39764  485.34845 ]\n\nResults for img2041.jpg:\nClass: 6, Score: 0.895, Box: [ 66.03506  106.140724 631.9394   516.2961  ]\n\nResults for img2047.jpg:\nClass: 6, Score: 0.998, Box: [130.82979 120.92493 502.628   564.1821 ]\n\nResults for img919.jpg:\nClass: 3, Score: 0.966, Box: [ 62.297585 156.67986  601.12006  556.5573  ]\n\nResults for img922.jpg:\nClass: 3, Score: 0.993, Box: [ 95.18184   55.958595 532.8798   600.2148  ]\n\nResults for img2827.jpg:\nClass: 8, Score: 0.996, Box: [ 49.55078 121.42185 615.3862  517.7924 ]\n\nResults for img2334.jpg:\nClass: 7, Score: 0.999, Box: [ 87.856346  15.204078 527.138    634.3465  ]\n\nResults for img118.jpg:\nClass: 1, Score: 0.991, Box: [175.17607 145.62471 470.98038 622.6633 ]\n\nResults for img920.jpg:\nClass: 3, Score: 0.729, Box: [ 73.03005   16.525587 476.41876  627.6167  ]\n\nResults for img1112.jpg:\nClass: 1, Score: 0.754, Box: [ 37.29607 112.51037 581.9915  433.7585 ]\nClass: 4, Score: 0.714, Box: [260.5482  249.20874 430.20493 369.97458]\nClass: 5, Score: 0.693, Box: [285.9627  253.12825 413.27692 361.79675]\n\nResults for img105.jpg:\nClass: 1, Score: 0.871, Box: [159.71368 112.64998 472.49023 624.2091 ]\nClass: 2, Score: 0.578, Box: [143.33736  96.08819 492.78888 629.7068 ]\n\nResults for img2044.jpg:\nClass: 6, Score: 0.958, Box: [155.53062   74.036316 403.19644  459.28488 ]\n\nResults for img1985.jpg:\nClass: 7, Score: 0.725, Box: [  0.      197.08601 618.54877 486.04443]\n\nResults for img1744.jpg:\nClass: 5, Score: 0.821, Box: [149.17485 286.18173 388.82785 521.07196]\n\nResults for img2326.jpg:\nClass: 7, Score: 0.998, Box: [182.75438  32.65774 512.1123  603.8079 ]\n\nResults for img2124.jpg:\nClass: 1, Score: 0.587, Box: [132.55734   23.204395 518.17456  546.612   ]\n\nResults for img1729.jpg:\nClass: 5, Score: 0.953, Box: [350.49664 269.07248 474.0279  428.75888]\n\nResults for img2117.jpg:\nClass: 1, Score: 0.981, Box: [  0.      152.57487 630.5389  504.14658]\n\nResults for img1987.jpg:\nClass: 5, Score: 0.941, Box: [200.7853 362.2242 386.5355 483.9335]\nClass: 5, Score: 0.756, Box: [288.32803 235.1428  418.17065 335.97333]\nClass: 4, Score: 0.514, Box: [289.9641  231.95557 420.5021  341.8021 ]\n\nResults for img103.jpg:\nClass: 1, Score: 0.997, Box: [ 14.471338 134.43587  598.7282   468.27206 ]\n\nResults for img755.jpg:\nClass: 3, Score: 0.995, Box: [ 28.792067  81.48504  586.7463   521.0298  ]\n\nResults for img1982.jpg:\nClass: 4, Score: 0.989, Box: [ 26.47207 233.16948 202.543   353.68463]\nClass: 4, Score: 0.980, Box: [198.77565 360.1578  438.0626  490.91367]\n\nResults for img2893.jpg:\nClass: 1, Score: 0.973, Box: [ 53.048706 200.84206  571.59766  449.02252 ]\n\nResults for img120.jpg:\nClass: 1, Score: 0.991, Box: [187.16315   28.761816 514.74695  612.9062  ]\n\nResults for img756.jpg:\nClass: 3, Score: 0.985, Box: [ 38.049267 133.08018  596.6778   493.58243 ]\n\nResults for img2820.jpg:\nClass: 8, Score: 0.992, Box: [182.60796 150.35776 427.20612 512.7617 ]\n\nResults for img2829.jpg:\nClass: 8, Score: 0.998, Box: [138.13222   92.858986 470.138    567.71564 ]\n\nResults for img112.jpg:\nClass: 1, Score: 0.830, Box: [141.78442   31.471949 475.81143  614.94025 ]\n\nResults for img622.jpg:\nClass: 3, Score: 0.975, Box: [120.89358  74.22556 441.58066 595.51184]\n\nResults for photo-collage.png:\nClass: 5, Score: 0.983, Box: [448.19434 746.9914  589.7642  895.86865]\nClass: 4, Score: 0.928, Box: [433.37793 264.10794 662.08405 474.56512]\nClass: 6, Score: 0.658, Box: [788.179   405.8826  989.43054 650.3193 ]\nClass: 6, Score: 0.630, Box: [787.9229  784.69806 991.8949  989.6758 ]\nClass: 1, Score: 0.594, Box: [ 56.100662   9.164112 350.07822  345.76883 ]\nClass: 6, Score: 0.567, Box: [ 781.40454  108.08204 1025.9838   288.88257]\n\nResults for img1735.jpg:\nClass: 4, Score: 0.927, Box: [365.94522 226.66162 493.0833  380.41928]\n\nResults for WhatsApp Image 2024-10-22 at 15.43.41_612db12b.jpg:\nClass: 5, Score: 0.984, Box: [343.20718 621.2147  557.15826 831.0488 ]\n\nResults for img2592.jpg:\nClass: 7, Score: 0.905, Box: [ 12.087036 232.08784  640.       510.00986 ]\n\nResults for img2120.jpg:\nClass: 7, Score: 0.852, Box: [ 15.648414 135.88286  637.25385  463.64926 ]\nClass: 6, Score: 0.665, Box: [ 21.975538 134.29172  639.95776  446.88217 ]\n\nResults for img115.jpg:\nClass: 1, Score: 0.979, Box: [ 91.04888   22.610035 444.01602  580.6011  ]\n\nResults for img2335.jpg:\nClass: 7, Score: 0.801, Box: [157.04118  68.20335 468.04465 531.58234]\nClass: 4, Score: 0.517, Box: [182.69212 134.79482 451.22803 529.86005]\n\nResults for img2824.jpg:\nClass: 8, Score: 0.975, Box: [  8.8594  190.63528 640.      523.6538 ]\n\nResults for img1741.jpg:\nClass: 5, Score: 0.985, Box: [315.83032 203.13348 523.2639  370.18784]\n\nResults for img2892.jpg:\nClass: 8, Score: 0.992, Box: [ 12.763745 168.29308  535.1767   525.229   ]\n\nResults for img585.jpg:\nClass: 1, Score: 0.927, Box: [116.44192  74.11924 459.96945 587.3669 ]\n\nResults for img2333.jpg:\nClass: 7, Score: 0.974, Box: [160.81496   0.      564.51654 617.8589 ]\n\nResults for img111.jpg:\nClass: 1, Score: 0.981, Box: [145.77856 100.22251 471.30255 595.34454]\n\nResults for img1124.jpg:\nClass: 5, Score: 0.948, Box: [449.49728 205.97623 550.9001  326.6235 ]\nClass: 4, Score: 0.900, Box: [121.0283  301.35532 299.25436 491.1522 ]\n\nResults for img590.jpg:\nClass: 8, Score: 0.998, Box: [ 38.162037 123.94991  581.30316  506.22345 ]\n\nResults for img1986.jpg:\nClass: 6, Score: 0.767, Box: [150.79404 103.73936 529.0022  563.36145]\nClass: 8, Score: 0.643, Box: [140.1191    82.008156 556.7152   591.748   ]\n\nResults for img2339.jpg:\nClass: 3, Score: 0.984, Box: [ 35.254223 130.39705  547.6528   471.21817 ]\n\nResults for img2826.jpg:\nClass: 8, Score: 0.998, Box: [176.22856  82.71907 493.13828 578.75433]\n\nResults for WhatsApp Image 2024-10-22 at 16.09.56_60471117.jpg:\nClass: 8, Score: 0.797, Box: [ 42.37921   0.      543.3016  933.3005 ]\nClass: 8, Score: 0.786, Box: [ 240.16396  896.3483   720.      1280.     ]\n\nResults for img2043.jpg:\nClass: 6, Score: 0.987, Box: [104.31714  87.7397  375.3523  454.12338]\n\nResults for img1639.jpg:\nClass: 7, Score: 0.913, Box: [ 15.458594 226.70348  624.39954  491.75937 ]\n\nResults for img1736.jpg:\nClass: 5, Score: 0.831, Box: [228.99866 274.8684  369.98822 471.48965]\nClass: 4, Score: 0.655, Box: [229.78555 279.26865 371.17062 479.47598]\n\nResults for img1640.jpg:\nClass: 7, Score: 0.808, Box: [ 87.05696 198.28954 606.77484 453.77066]\nClass: 4, Score: 0.610, Box: [155.83379 203.52391 565.98444 471.25128]\n\nResults for img1745.jpg:\nClass: 5, Score: 0.937, Box: [136.447   292.8998  290.30896 458.45303]\nClass: 3, Score: 0.645, Box: [170.80217 165.50475 561.67426 484.0899 ]\n\nResults for img2316.jpg:\nClass: 7, Score: 0.998, Box: [149.11171   0.      509.42657 640.     ]\n\nResults for img2038.jpg:\nClass: 6, Score: 0.992, Box: [108.95396 182.51465 505.2048  442.542  ]\n\nResults for img2051.jpg:\nClass: 6, Score: 0.991, Box: [122.831825 173.92538  514.6996   418.2063  ]\n\nResults for img630.jpg:\nClass: 3, Score: 0.987, Box: [ 28.29602 113.46509 605.45245 523.7293 ]\n\nResults for img628.jpg:\nClass: 3, Score: 0.979, Box: [121.07269  44.17771 501.6568  610.3493 ]\n\nResults for img2048.jpg:\nClass: 6, Score: 0.984, Box: [140.71092  62.05835 467.40598 603.75433]\n\nResults for img2039.jpg:\nClass: 6, Score: 0.996, Box: [ 98.4537  184.20758 436.9484  418.74307]\n\nResults for img1981.jpg:\nClass: 4, Score: 0.987, Box: [255.0929  349.2041  465.01767 470.25363]\n\nResults for WhatsApp Image 2024-10-22 at 15.43.32_fa8928a5.jpg:\nClass: 5, Score: 0.943, Box: [174.17711 308.00604 514.963   601.38   ]\nClass: 8, Score: 0.573, Box: [   0.      324.8382  791.0651 1119.7434]\n\nResults for img923.jpg:\nClass: 3, Score: 0.990, Box: [  7.8950686  69.11765   610.1234    489.55283  ]\n\nResults for img925.jpg:\nClass: 3, Score: 0.878, Box: [ 89.128006  11.959546 515.5591   626.03577 ]\n\nResults for img1742.jpg:\nClass: 5, Score: 0.976, Box: [228.48729 399.42383 348.79706 530.4716 ]\nClass: 3, Score: 0.627, Box: [165.94403   0.      525.0377  579.94464]\n\nResults for img1122.jpg:\nClass: 4, Score: 0.964, Box: [106.41335 297.1702  330.24057 457.8942 ]\n\nResults for img1637.jpg:\nClass: 4, Score: 0.985, Box: [ 76.881836 165.0485   364.99796  270.66876 ]\nClass: 1, Score: 0.924, Box: [ 14.568311 145.7975   636.7202   489.52316 ]\n\nResults for img1980.jpg:\nClass: 1, Score: 0.980, Box: [ 96.33604 198.72318 579.9979  524.25   ]\nClass: 5, Score: 0.958, Box: [119.89004 235.19547 231.89351 350.0978 ]\n\nResults for img2037.jpg:\nClass: 6, Score: 0.989, Box: [141.9631   31.69314 477.13663 607.88116]\n\nResults for img1648.jpg:\nClass: 7, Score: 0.898, Box: [  8.31897 187.29698 632.1543  447.84048]\n\nResults for WhatsApp Image 2024-10-22 at 16.08.28_0bd91554.jpg:\nClass: 1, Score: 0.511, Box: [8.4537700e-02 0.0000000e+00 9.2382080e+02 9.6000006e+02]\n\nResults for img1739.jpg:\nClass: 5, Score: 0.964, Box: [183.55077 208.22093 348.61606 384.14316]\nClass: 5, Score: 0.910, Box: [195.84262 396.03973 358.23004 537.4626 ]\nClass: 4, Score: 0.538, Box: [304.93942  93.59695 452.5085  239.288  ]\n\nResults for img2042.jpg:\nClass: 6, Score: 0.994, Box: [143.1159    45.334084 486.4235   580.7543  ]\n\nResults for img2342.jpg:\nClass: 7, Score: 0.998, Box: [136.17477  30.40459 496.194   627.10895]\n\nResults for img1645.jpg:\nClass: 7, Score: 0.929, Box: [ 17.402588 113.54335  629.2302   468.2206  ]\n\nResults for img2821.jpg:\nClass: 8, Score: 0.982, Box: [143.96571 100.75266 539.2642  449.1379 ]\n\nResults for img121.jpg:\nClass: 2, Score: 0.806, Box: [127.3077  108.93125 491.48453 604.45184]\n\nResults for img1118.jpg:\nClass: 5, Score: 0.965, Box: [313.69235 199.84892 415.781   298.5289 ]\nClass: 2, Score: 0.679, Box: [145.45848  64.85345 469.11505 587.33685]\n\nResults for img2589.jpg:\nClass: 7, Score: 0.807, Box: [ 22.507887 167.23802  553.7668   400.87674 ]\n\nResults for WhatsApp Image 2024-10-22 at 16.08.26_d026a843.jpg:\nClass: 4, Score: 0.901, Box: [ 70.51922 233.68462 467.31    675.38934]\n\nResults for img1123.jpg:\nClass: 4, Score: 0.992, Box: [185.50969 158.04602 500.95734 357.8913 ]\n\nResults for img1113.jpg:\nClass: 4, Score: 0.853, Box: [ 58.005188 129.00162  351.39508  331.97418 ]\nClass: 4, Score: 0.806, Box: [  2.0217042 324.65268   174.87996   449.4738   ]\n\nResults for img2050.jpg:\nClass: 6, Score: 0.995, Box: [115.6845    85.939285 532.8498   571.1437  ]\n\nResults for img629.jpg:\nClass: 3, Score: 0.962, Box: [127.53799   31.457716 532.98285  616.62885 ]\n\nResults for img2122.jpg:\nClass: 6, Score: 0.998, Box: [ 28.500341 182.62753  564.2197   466.76685 ]\n\nResults for img2325.jpg:\nClass: 7, Score: 0.996, Box: [  2.0480225  97.33303   619.3984    516.4461   ]\n\nResults for img1632.jpg:\nClass: 4, Score: 0.989, Box: [385.2229  310.73532 575.04047 439.64923]\nClass: 1, Score: 0.867, Box: [ 21.7344  166.88611 618.9643  487.06955]\n\nResults for img2594.jpg:\nClass: 7, Score: 0.957, Box: [ 30.69104 223.50484 622.6569  477.99698]\n\nResults for img2052.jpg:\nClass: 6, Score: 0.561, Box: [ 49.08777  85.77246 625.1935  483.17294]\n\nResults for img108.jpg:\nClass: 1, Score: 0.992, Box: [131.58714   21.035181 527.6832   626.5366  ]\n\nResults for img2839.jpg:\nClass: 8, Score: 0.994, Box: [160.3328  202.54063 533.4515  450.85098]\n\nResults for WhatsApp Image 2024-10-22 at 15.43.34_788c8848.jpg:\nClass: 5, Score: 0.891, Box: [208.2376  453.25314 576.63556 685.1754 ]\n\nResults for img1983.jpg:\nClass: 4, Score: 0.798, Box: [106.022705 306.03305  302.42987  474.56625 ]\n\nResults for img2327.jpg:\nClass: 7, Score: 0.995, Box: [  0.      182.59433 587.9749  488.65625]\n\nResults for img1633.jpg:\nClass: 4, Score: 0.930, Box: [ 20.889082 251.4459   157.5065   395.6013  ]\nClass: 7, Score: 0.784, Box: [ 60.394436 170.34552  601.52155  512.6416  ]\n\nResults for img2832.jpg:\nClass: 8, Score: 0.997, Box: [170.92401  103.749084 474.7941   492.85455 ]\n\nResults for img1110.jpg:\nClass: 4, Score: 0.780, Box: [119.08909 250.85582 477.80527 479.79718]\nClass: 5, Score: 0.628, Box: [136.30232 246.3433  468.0371  487.18164]\n\nResults for img1649.jpg:\nClass: 4, Score: 0.994, Box: [ 19.94491 231.13737 310.39633 480.02707]\nClass: 7, Score: 0.780, Box: [ 90.23574 164.23398 614.81506 476.5922 ]\n\nResults for img927.jpg:\nClass: 3, Score: 0.963, Box: [ 80.8511    46.299732 516.67865  617.8019  ]\n\nResults for img109.jpg:\nClass: 7, Score: 0.986, Box: [ 60.47693 173.99548 598.7358  545.1181 ]\n\nResults for img2593.jpg:\nClass: 7, Score: 0.991, Box: [ 12.124951 214.18028  613.025    449.35205 ]\n\nResults for img2330.jpg:\nClass: 7, Score: 0.998, Box: [ 32.649635 132.5687   616.9704   494.60342 ]\n\nResults for img2825.jpg:\nClass: 8, Score: 0.998, Box: [ 70.7042  194.72693 524.81445 465.5165 ]\n\nResults for img2046.jpg:\nClass: 6, Score: 0.994, Box: [136.74323  46.99065 483.4315  585.404  ]\n\nResults for img2119.jpg:\nClass: 6, Score: 0.994, Box: [ 21.69729 174.8367  573.97943 500.40536]\n\nResults for img627.jpg:\nClass: 3, Score: 0.976, Box: [207.6936   85.84951 499.00436 565.8659 ]\n\nResults for img1126.jpg:\nClass: 4, Score: 0.881, Box: [181.66048 268.77057 276.1598  387.12204]\n\nResults for img2040.jpg:\nClass: 6, Score: 0.908, Box: [142.60909   79.234375 447.86905  595.90204 ]\n\nResults for img2341.jpg:\nClass: 7, Score: 0.997, Box: [168.04138   11.511475 507.2758   614.6921  ]\n\nResults for img753.jpg:\nClass: 3, Score: 0.988, Box: [ 98.63423  69.23069 509.22076 626.1002 ]\n\nResults for img1125.jpg:\nClass: 4, Score: 0.952, Box: [178.44142 210.56505 395.57916 523.4797 ]\n\nResults for img750.jpg:\nClass: 3, Score: 0.994, Box: [111.15423   16.432032 576.708    591.1464  ]\n\nResults for img1116.jpg:\nClass: 5, Score: 0.861, Box: [265.85703 184.90283 463.95676 323.65045]\n\nResults for img1121.jpg:\nClass: 4, Score: 0.987, Box: [208.00235   77.686905 413.92636  234.50117 ]\nClass: 1, Score: 0.947, Box: [182.25053 108.7927  496.89963 608.763  ]\n\nResults for img2831.jpg:\nClass: 8, Score: 0.933, Box: [ 47.4021  115.04378 552.4999  593.675  ]\n\nResults for img2891.jpg:\nClass: 1, Score: 0.978, Box: [ 57.451538 209.00114  636.77606  513.61127 ]\n\nResults for WhatsApp Image 2024-10-22 at 16.08.27_8b40b39e.jpg:\nClass: 5, Score: 0.978, Box: [ 28.830685 498.2822   167.23413  632.9139  ]\nClass: 5, Score: 0.945, Box: [868.15515 228.83026 978.3146  328.80624]\nClass: 5, Score: 0.821, Box: [1126.8469  564.1618 1280.      700.6745]\nClass: 1, Score: 0.548, Box: [  0.        0.      471.96573 511.2945 ]\nClass: 5, Score: 0.504, Box: [ 944.8096  484.7692 1053.8462  592.4915]\n\nResults for img114.jpg:\nClass: 1, Score: 0.802, Box: [ 52.315968 167.23587  640.       530.65967 ]\nClass: 2, Score: 0.731, Box: [ 68.20135 150.04578 640.      532.7835 ]\n\nResults for img754.jpg:\nClass: 3, Score: 0.962, Box: [101.496056  70.61824  477.7509   577.97736 ]\n\nResults for img106.jpg:\nClass: 1, Score: 0.982, Box: [146.39948   16.794703 511.3023   611.1702  ]\n\nResults for img748.jpg:\nClass: 3, Score: 0.977, Box: [ 25.788649  86.40989  583.25214  509.17245 ]\n\nResults for img1642.jpg:\nClass: 7, Score: 0.657, Box: [ 22.352589 211.12935  633.59735  491.25684 ]\n\nResults for img588.jpg:\nClass: 2, Score: 0.848, Box: [159.56375 165.8708  492.23663 493.8069 ]\n\nResults for img2321.jpg:\nClass: 7, Score: 0.998, Box: [ 14.0630865 129.23799   527.9578    436.79892  ]\n\nResults for img1641.jpg:\nClass: 4, Score: 0.995, Box: [ 47.26576 204.68155 273.93115 311.68204]\nClass: 4, Score: 0.938, Box: [153.47318 297.1629  355.52664 442.99908]\n\nResults for img1731.jpg:\nClass: 5, Score: 0.980, Box: [171.87614 185.72566 347.748   397.07034]\n\nResults for img624.jpg:\nClass: 3, Score: 0.981, Box: [ 84.48415 163.0064  586.05096 466.48184]\n\nResults for img107.jpg:\nClass: 1, Score: 0.997, Box: [141.88211   26.115162 535.9824   638.964   ]\n\nResults for img1740.jpg:\nClass: 5, Score: 0.968, Box: [353.62955 204.55644 553.5936  378.04367]\n\nResults for img623.jpg:\nClass: 3, Score: 0.871, Box: [183.02242   77.498955 495.1262   562.98047 ]\n\nResults for WhatsApp Image 2024-10-22 at 16.08.26_f2b7c86c.jpg:\nClass: 4, Score: 0.932, Box: [456.96777 342.56064 593.3946  510.69565]\nClass: 5, Score: 0.898, Box: [235.07703   24.869387 313.4138   150.40614 ]\nClass: 4, Score: 0.893, Box: [492.39038   69.986725 611.47455  293.3554  ]\nClass: 4, Score: 0.873, Box: [140.10796 407.65866 327.93027 642.9844 ]\nClass: 1, Score: 0.535, Box: [ 16.279423  68.4478   168.41231  340.4934  ]\n\nResults for img2322.jpg:\nClass: 7, Score: 0.998, Box: [123.26837  17.66106 463.62228 613.1759 ]\n\nResults for img2591.jpg:\nClass: 7, Score: 0.949, Box: [  2.7177002 141.4594    621.3804    490.48935  ]\n\nResults for img2340.jpg:\nClass: 7, Score: 0.845, Box: [145.6729    57.867138 470.4482   606.6869  ]\n\nResults for img2331.jpg:\nClass: 7, Score: 0.990, Box: [ 21.500929 159.89645  595.7944   462.48264 ]\n\nResults for img2123.jpg:\nClass: 6, Score: 0.963, Box: [ 28.958471 191.46498  616.8921   451.0647  ]\n\nResults for img116.jpg:\nClass: 4, Score: 0.937, Box: [217.68413 204.84642 366.24503 379.47128]\n\nResults for img2319.jpg:\nClass: 7, Score: 0.998, Box: [145.51141   30.100317 512.4993   591.5151  ]\n\nResults for img2338.jpg:\nClass: 7, Score: 0.976, Box: [181.35219 166.08263 482.15698 562.4455 ]\n\nResults for img589.jpg:\nClass: 2, Score: 0.934, Box: [ 92.929565  13.379956 511.21973  600.68896 ]\n\nResults for img2121.jpg:\nClass: 6, Score: 0.987, Box: [151.57185 100.11482 473.9077  567.54877]\n\nResults for img2828.jpg:\nClass: 8, Score: 0.997, Box: [142.50151  49.47129 586.6434  612.3338 ]\n\nResults for img117.jpg:\nClass: 1, Score: 0.991, Box: [ 77.010635  50.93467  458.4271   578.80316 ]\n\nResults for img110.jpg:\nClass: 1, Score: 0.946, Box: [110.46411   56.177883 504.17877  636.448   ]\n\nResults for img2890.jpg:\nClass: 8, Score: 0.997, Box: [157.56456 104.47681 445.68292 548.6781 ]\n\nResults for WhatsApp Image 2024-10-22 at 15.43.33_b221e73d.jpg:\nClass: 5, Score: 0.980, Box: [140.64732 653.03516 348.24365 813.50714]\nClass: 5, Score: 0.952, Box: [566.9304 616.9877 742.9504 773.83  ]\nClass: 5, Score: 0.913, Box: [449.37906 682.2817  567.46326 783.5007 ]\n\nResults for img1114.jpg:\nClass: 4, Score: 0.799, Box: [120.03405 280.04037 324.01413 530.1371 ]\n\nResults for img1644.jpg:\nClass: 4, Score: 0.994, Box: [ 10.731689 176.43173  258.79572  402.44458 ]\nClass: 1, Score: 0.920, Box: [ 40.087746 149.0194   636.577    528.9893  ]\nClass: 4, Score: 0.679, Box: [ 28.246534 169.78618  396.60278  467.37408 ]\n\nResults for img2324.jpg:\nClass: 7, Score: 0.998, Box: [ 27.799072 134.76785  640.       519.16595 ]\n\nResults for img1638.jpg:\nClass: 4, Score: 0.548, Box: [ 82.55901 326.61633 268.17172 455.40738]\n\nResults for img2337.jpg:\nClass: 7, Score: 0.997, Box: [ 33.98367 113.01221 623.77    485.13654]\n\nResults for img1643.jpg:\nClass: 4, Score: 0.997, Box: [ 17.828663 200.13301  307.8534   364.9571  ]\n\nResults for img2837.jpg:\nClass: 8, Score: 0.869, Box: [134.27464 119.60027 451.49463 602.0329 ]\n\nResults for img584.jpg:\nClass: 8, Score: 0.997, Box: [ 32.811718 144.74849  481.93658  503.7979  ]\n\nResults for img586.jpg:\nClass: 2, Score: 0.961, Box: [ 76.4929    47.843334 498.37823  581.2593  ]\n\nResults for img2889.jpg:\nClass: 6, Score: 0.988, Box: [129.8634    12.184766 453.6504   582.6452  ]\n\nResults for img1647.jpg:\nClass: 4, Score: 0.995, Box: [368.77368 167.06474 631.9903  401.6834 ]\nClass: 7, Score: 0.639, Box: [  0.      166.41087 506.358   489.03726]\n\nResults for img583.jpg:\nClass: 8, Score: 0.993, Box: [104.5851    42.403442 496.34665  561.5892  ]\n\nResults for img1636.jpg:\nClass: 4, Score: 0.975, Box: [347.66998 246.94969 560.8481  368.57806]\n\nResults for img2323.jpg:\nClass: 7, Score: 0.989, Box: [ 17.789356 112.116005 572.40845  445.18878 ]\n\nResults for img1120.jpg:\nClass: 5, Score: 0.982, Box: [345.48593 389.36307 475.37228 504.244  ]\n\nResults for img587.jpg:\nClass: 2, Score: 0.996, Box: [ 14.301806  93.10359  601.18665  506.5082  ]\n\nResults for img1117.jpg:\nClass: 5, Score: 0.743, Box: [304.2272  244.09761 500.8425  419.71426]\n\nResults for img2830.jpg:\nClass: 8, Score: 0.995, Box: [1.4466278e+02 1.0986328e-01 5.4738232e+02 5.9704535e+02]\n\nResults for WhatsApp Image 2024-10-22 at 16.09.56_80b19fb3.jpg:\nClass: 8, Score: 0.789, Box: [  5.4379325   0.        746.5873    920.0445   ]\nClass: 4, Score: 0.736, Box: [640.7826  164.28337 875.207   383.72467]\n\nResults for img104.jpg:\nClass: 1, Score: 0.991, Box: [145.20593   23.856398 549.1598   601.86115 ]\n\nResults for img2818.jpg:\nClass: 8, Score: 0.994, Box: [ 36.683277 115.00256  635.5786   525.3503  ]\n\nResults for img2320.jpg:\nClass: 7, Score: 0.996, Box: [ 13.735522 150.96161  583.37463  531.48175 ]\n\nResults for img1115.jpg:\nClass: 4, Score: 0.992, Box: [161.56541 213.04753 311.72363 373.5317 ]\nClass: 1, Score: 0.836, Box: [143.15092   48.669704 506.87726  614.4294  ]\n\nResults for img2036.jpg:\nClass: 6, Score: 0.984, Box: [180.1397  134.55533 480.7517  562.4968 ]\n\nResults for img626.jpg:\nClass: 3, Score: 0.971, Box: [204.83052  88.15081 500.26666 565.26965]\n\nResults for img2053.jpg:\nClass: 6, Score: 0.980, Box: [127.49096 138.10591 417.43893 524.75635]\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as T         \nfrom PIL import Image, ImageDraw, ImageFont\nimport os\n\n# Define test images folder and output folder\ntest_images_folder = \"/kaggle/input/fasterrcnntealeaf/dataset/test\"\noutput_folder = \"/kaggle/working/predicted_images\"\n\n# Create the output directory if it doesn't exist\nos.makedirs(output_folder, exist_ok=True)\n\n# Define transformation\ntransform = T.Compose([\n    T.ToTensor(),  # Convert image to tensor\n])\n\n# Define class names (Modify based on your dataset)\nclass_names = [\"background\",\"Algal Leaf Rust\",\"Bug Eaten\",\"Healthy\",\"Leaf Blight\",\"Leaf Spot\",\"Nutrition Deficiency\",\"Red Spider Mite\",\"Tea Mosquito Bug\"]  # Update accordingly\n\n# Function to draw bounding boxes and save images\ndef visualize_and_save_predictions(image_path, boxes, labels, scores, threshold=0.5):\n    \"\"\"\n    Draws bounding boxes on the image and saves it.\n    \"\"\"\n    image = Image.open(image_path).convert(\"RGB\")\n    draw = ImageDraw.Draw(image)\n    \n    try:\n        font = ImageFont.truetype(\"arial.ttf\", 20)  # Try Arial font\n    except:\n        font = ImageFont.load_default()  # Default font if Arial not available\n\n    for i in range(len(scores)):\n        if scores[i] >= threshold:\n            box = boxes[i]\n            label = labels[i]\n            score = scores[i]\n            class_name = class_names[label]\n\n            # Draw bounding box\n            draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline=\"red\", width=3)\n\n            # Draw label text\n            text = f\"{class_name}: {score:.2f}\"\n            draw.text((box[0], box[1] - 10), text, fill=\"red\", font=font)\n\n    # Save the image with predictions\n    output_image_path = os.path.join(output_folder, os.path.basename(image_path))\n    image.save(output_image_path)\n    print(f\"Saved: {output_image_path}\")\n\n# Load test images\ntest_images = [f for f in os.listdir(test_images_folder) if f.endswith(('.jpg', '.png', '.jpeg'))]\n\n# Process and test each image\nfor image_name in test_images:\n    image_path = os.path.join(test_images_folder, image_name)\n    \n    # Load and preprocess the image\n    image = Image.open(image_path).convert(\"RGB\")\n    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n    \n    # Make prediction\n    with torch.no_grad():\n        predictions = model(image_tensor)\n\n    # Extract results\n    boxes = predictions[0]['boxes'].cpu().numpy()  # Bounding boxes\n    scores = predictions[0]['scores'].cpu().numpy()  # Confidence scores\n    labels = predictions[0]['labels'].cpu().numpy()  # Class labels\n\n    # Save the image with drawn predictions\n    visualize_and_save_predictions(image_path, boxes, labels, scores)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T13:01:40.238926Z","iopub.execute_input":"2025-02-12T13:01:40.239268Z","iopub.status.idle":"2025-02-12T13:09:08.611667Z","shell.execute_reply.started":"2025-02-12T13:01:40.239240Z","shell.execute_reply":"2025-02-12T13:09:08.610729Z"}},"outputs":[{"name":"stdout","text":"Saved: /kaggle/working/predicted_images/img1738.jpg\nSaved: /kaggle/working/predicted_images/img1119.jpg\nSaved: /kaggle/working/predicted_images/img1743.jpg\nSaved: /kaggle/working/predicted_images/img2823.jpg\nSaved: /kaggle/working/predicted_images/img2895.jpg\nSaved: /kaggle/working/predicted_images/img2588.jpg\nSaved: /kaggle/working/predicted_images/img2328.jpg\nSaved: /kaggle/working/predicted_images/img749.jpg\nSaved: /kaggle/working/predicted_images/img752.jpg\nSaved: /kaggle/working/predicted_images/img1984.jpg\nSaved: /kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 15.43.32_bfe9e469.jpg\nSaved: /kaggle/working/predicted_images/img2590.jpg\nSaved: /kaggle/working/predicted_images/img1730.jpg\nSaved: /kaggle/working/predicted_images/img1646.jpg\nSaved: /kaggle/working/predicted_images/img921.jpg\nSaved: /kaggle/working/predicted_images/img2329.jpg\nSaved: /kaggle/working/predicted_images/img591.jpg\nSaved: /kaggle/working/predicted_images/img2317.jpg\nSaved: /kaggle/working/predicted_images/img2897.jpg\nSaved: /kaggle/working/predicted_images/img119.jpg\nSaved: /kaggle/working/predicted_images/img2587.jpg\nSaved: /kaggle/working/predicted_images/img2336.jpg\nSaved: /kaggle/working/predicted_images/img2925.jpg\nSaved: /kaggle/working/predicted_images/img1988.jpg\nSaved: /kaggle/working/predicted_images/img1732.jpg\nSaved: /kaggle/working/predicted_images/img2835.jpg\nSaved: /kaggle/working/predicted_images/img1127.jpg\nSaved: /kaggle/working/predicted_images/img1111.jpg\nSaved: /kaggle/working/predicted_images/img2896.jpg\nSaved: /kaggle/working/predicted_images/img1734.jpg\nSaved: /kaggle/working/predicted_images/img1733.jpg\nSaved: /kaggle/working/predicted_images/img1635.jpg\nSaved: /kaggle/working/predicted_images/img2118.jpg\nSaved: /kaggle/working/predicted_images/img2838.jpg\nSaved: /kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 16.10.30_939018d5.jpg\nSaved: /kaggle/working/predicted_images/img924.jpg\nSaved: /kaggle/working/predicted_images/img2318.jpg\nSaved: /kaggle/working/predicted_images/img1737.jpg\nSaved: /kaggle/working/predicted_images/img1728.jpg\nSaved: /kaggle/working/predicted_images/img2125.jpg\nSaved: /kaggle/working/predicted_images/img2894.jpg\nSaved: /kaggle/working/predicted_images/img625.jpg\nSaved: /kaggle/working/predicted_images/1.jpg\nSaved: /kaggle/working/predicted_images/img751.jpg\nSaved: /kaggle/working/predicted_images/img1634.jpg\nSaved: /kaggle/working/predicted_images/img2045.jpg\nSaved: /kaggle/working/predicted_images/img926.jpg\nSaved: /kaggle/working/predicted_images/img2586.jpg\nSaved: /kaggle/working/predicted_images/img2332.jpg\nSaved: /kaggle/working/predicted_images/img2049.jpg\nSaved: /kaggle/working/predicted_images/img2834.jpg\nSaved: /kaggle/working/predicted_images/img2041.jpg\nSaved: /kaggle/working/predicted_images/img2047.jpg\nSaved: /kaggle/working/predicted_images/img919.jpg\nSaved: /kaggle/working/predicted_images/img922.jpg\nSaved: /kaggle/working/predicted_images/img2827.jpg\nSaved: /kaggle/working/predicted_images/img2334.jpg\nSaved: /kaggle/working/predicted_images/img118.jpg\nSaved: /kaggle/working/predicted_images/img920.jpg\nSaved: /kaggle/working/predicted_images/img1112.jpg\nSaved: /kaggle/working/predicted_images/img105.jpg\nSaved: /kaggle/working/predicted_images/img2044.jpg\nSaved: /kaggle/working/predicted_images/img1985.jpg\nSaved: /kaggle/working/predicted_images/img1744.jpg\nSaved: /kaggle/working/predicted_images/img2326.jpg\nSaved: /kaggle/working/predicted_images/img2124.jpg\nSaved: /kaggle/working/predicted_images/img1729.jpg\nSaved: /kaggle/working/predicted_images/img2117.jpg\nSaved: /kaggle/working/predicted_images/img1987.jpg\nSaved: /kaggle/working/predicted_images/img103.jpg\nSaved: /kaggle/working/predicted_images/img755.jpg\nSaved: /kaggle/working/predicted_images/img1982.jpg\nSaved: /kaggle/working/predicted_images/img2893.jpg\nSaved: /kaggle/working/predicted_images/img120.jpg\nSaved: /kaggle/working/predicted_images/img756.jpg\nSaved: /kaggle/working/predicted_images/img2820.jpg\nSaved: /kaggle/working/predicted_images/img2829.jpg\nSaved: /kaggle/working/predicted_images/img112.jpg\nSaved: /kaggle/working/predicted_images/img622.jpg\nSaved: /kaggle/working/predicted_images/photo-collage.png\nSaved: /kaggle/working/predicted_images/img1735.jpg\nSaved: /kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 15.43.41_612db12b.jpg\nSaved: /kaggle/working/predicted_images/img2592.jpg\nSaved: /kaggle/working/predicted_images/img2120.jpg\nSaved: /kaggle/working/predicted_images/img115.jpg\nSaved: /kaggle/working/predicted_images/img2335.jpg\nSaved: /kaggle/working/predicted_images/img2824.jpg\nSaved: /kaggle/working/predicted_images/img1741.jpg\nSaved: /kaggle/working/predicted_images/img2892.jpg\nSaved: /kaggle/working/predicted_images/img585.jpg\nSaved: /kaggle/working/predicted_images/img2333.jpg\nSaved: /kaggle/working/predicted_images/img111.jpg\nSaved: /kaggle/working/predicted_images/img1124.jpg\nSaved: /kaggle/working/predicted_images/img590.jpg\nSaved: /kaggle/working/predicted_images/img1986.jpg\nSaved: /kaggle/working/predicted_images/img2339.jpg\nSaved: /kaggle/working/predicted_images/img2826.jpg\nSaved: /kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 16.09.56_60471117.jpg\nSaved: /kaggle/working/predicted_images/img2043.jpg\nSaved: /kaggle/working/predicted_images/img1639.jpg\nSaved: /kaggle/working/predicted_images/img1736.jpg\nSaved: /kaggle/working/predicted_images/img1640.jpg\nSaved: /kaggle/working/predicted_images/img1745.jpg\nSaved: /kaggle/working/predicted_images/img2316.jpg\nSaved: /kaggle/working/predicted_images/img2038.jpg\nSaved: /kaggle/working/predicted_images/img2051.jpg\nSaved: /kaggle/working/predicted_images/img630.jpg\nSaved: /kaggle/working/predicted_images/img628.jpg\nSaved: /kaggle/working/predicted_images/img2048.jpg\nSaved: /kaggle/working/predicted_images/img2039.jpg\nSaved: /kaggle/working/predicted_images/img1981.jpg\nSaved: /kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 15.43.32_fa8928a5.jpg\nSaved: /kaggle/working/predicted_images/img923.jpg\nSaved: /kaggle/working/predicted_images/img925.jpg\nSaved: /kaggle/working/predicted_images/img1742.jpg\nSaved: /kaggle/working/predicted_images/img1122.jpg\nSaved: /kaggle/working/predicted_images/img1637.jpg\nSaved: /kaggle/working/predicted_images/img1980.jpg\nSaved: /kaggle/working/predicted_images/img2037.jpg\nSaved: /kaggle/working/predicted_images/img1648.jpg\nSaved: /kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 16.08.28_0bd91554.jpg\nSaved: /kaggle/working/predicted_images/img1739.jpg\nSaved: /kaggle/working/predicted_images/img2042.jpg\nSaved: /kaggle/working/predicted_images/img2342.jpg\nSaved: /kaggle/working/predicted_images/img1645.jpg\nSaved: /kaggle/working/predicted_images/img2821.jpg\nSaved: /kaggle/working/predicted_images/img121.jpg\nSaved: /kaggle/working/predicted_images/img1118.jpg\nSaved: /kaggle/working/predicted_images/img2589.jpg\nSaved: /kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 16.08.26_d026a843.jpg\nSaved: /kaggle/working/predicted_images/img1123.jpg\nSaved: /kaggle/working/predicted_images/img1113.jpg\nSaved: /kaggle/working/predicted_images/img2050.jpg\nSaved: /kaggle/working/predicted_images/img629.jpg\nSaved: /kaggle/working/predicted_images/img2122.jpg\nSaved: /kaggle/working/predicted_images/img2325.jpg\nSaved: /kaggle/working/predicted_images/img1632.jpg\nSaved: /kaggle/working/predicted_images/img2594.jpg\nSaved: /kaggle/working/predicted_images/img2052.jpg\nSaved: /kaggle/working/predicted_images/img108.jpg\nSaved: /kaggle/working/predicted_images/img2839.jpg\nSaved: /kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 15.43.34_788c8848.jpg\nSaved: /kaggle/working/predicted_images/img1983.jpg\nSaved: /kaggle/working/predicted_images/img2327.jpg\nSaved: /kaggle/working/predicted_images/img1633.jpg\nSaved: /kaggle/working/predicted_images/img2832.jpg\nSaved: /kaggle/working/predicted_images/img1110.jpg\nSaved: /kaggle/working/predicted_images/img1649.jpg\nSaved: /kaggle/working/predicted_images/img927.jpg\nSaved: /kaggle/working/predicted_images/img109.jpg\nSaved: /kaggle/working/predicted_images/img2593.jpg\nSaved: /kaggle/working/predicted_images/img2330.jpg\nSaved: /kaggle/working/predicted_images/img2825.jpg\nSaved: /kaggle/working/predicted_images/img2046.jpg\nSaved: /kaggle/working/predicted_images/img2119.jpg\nSaved: /kaggle/working/predicted_images/img627.jpg\nSaved: /kaggle/working/predicted_images/img1126.jpg\nSaved: /kaggle/working/predicted_images/img2040.jpg\nSaved: /kaggle/working/predicted_images/img2341.jpg\nSaved: /kaggle/working/predicted_images/img753.jpg\nSaved: /kaggle/working/predicted_images/img1125.jpg\nSaved: /kaggle/working/predicted_images/img750.jpg\nSaved: /kaggle/working/predicted_images/img1116.jpg\nSaved: /kaggle/working/predicted_images/img1121.jpg\nSaved: /kaggle/working/predicted_images/img2831.jpg\nSaved: /kaggle/working/predicted_images/img2891.jpg\nSaved: /kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 16.08.27_8b40b39e.jpg\nSaved: /kaggle/working/predicted_images/img114.jpg\nSaved: /kaggle/working/predicted_images/img754.jpg\nSaved: /kaggle/working/predicted_images/img106.jpg\nSaved: /kaggle/working/predicted_images/img748.jpg\nSaved: /kaggle/working/predicted_images/img1642.jpg\nSaved: /kaggle/working/predicted_images/img588.jpg\nSaved: /kaggle/working/predicted_images/img2321.jpg\nSaved: /kaggle/working/predicted_images/img1641.jpg\nSaved: /kaggle/working/predicted_images/img1731.jpg\nSaved: /kaggle/working/predicted_images/img624.jpg\nSaved: /kaggle/working/predicted_images/img107.jpg\nSaved: /kaggle/working/predicted_images/img1740.jpg\nSaved: /kaggle/working/predicted_images/img623.jpg\nSaved: /kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 16.08.26_f2b7c86c.jpg\nSaved: /kaggle/working/predicted_images/img2322.jpg\nSaved: /kaggle/working/predicted_images/img2591.jpg\nSaved: /kaggle/working/predicted_images/img2340.jpg\nSaved: /kaggle/working/predicted_images/img2331.jpg\nSaved: /kaggle/working/predicted_images/img2123.jpg\nSaved: /kaggle/working/predicted_images/img116.jpg\nSaved: /kaggle/working/predicted_images/img2319.jpg\nSaved: /kaggle/working/predicted_images/img2338.jpg\nSaved: /kaggle/working/predicted_images/img589.jpg\nSaved: /kaggle/working/predicted_images/img2121.jpg\nSaved: /kaggle/working/predicted_images/img2828.jpg\nSaved: /kaggle/working/predicted_images/img117.jpg\nSaved: /kaggle/working/predicted_images/img110.jpg\nSaved: /kaggle/working/predicted_images/img2890.jpg\nSaved: /kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 15.43.33_b221e73d.jpg\nSaved: /kaggle/working/predicted_images/img1114.jpg\nSaved: /kaggle/working/predicted_images/img1644.jpg\nSaved: /kaggle/working/predicted_images/img2324.jpg\nSaved: /kaggle/working/predicted_images/img1638.jpg\nSaved: /kaggle/working/predicted_images/img2337.jpg\nSaved: /kaggle/working/predicted_images/img1643.jpg\nSaved: /kaggle/working/predicted_images/img2837.jpg\nSaved: /kaggle/working/predicted_images/img584.jpg\nSaved: /kaggle/working/predicted_images/img586.jpg\nSaved: /kaggle/working/predicted_images/img2889.jpg\nSaved: /kaggle/working/predicted_images/img1647.jpg\nSaved: /kaggle/working/predicted_images/img583.jpg\nSaved: /kaggle/working/predicted_images/img1636.jpg\nSaved: /kaggle/working/predicted_images/img2323.jpg\nSaved: /kaggle/working/predicted_images/img1120.jpg\nSaved: /kaggle/working/predicted_images/img587.jpg\nSaved: /kaggle/working/predicted_images/img1117.jpg\nSaved: /kaggle/working/predicted_images/img2830.jpg\nSaved: /kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 16.09.56_80b19fb3.jpg\nSaved: /kaggle/working/predicted_images/img104.jpg\nSaved: /kaggle/working/predicted_images/img2818.jpg\nSaved: /kaggle/working/predicted_images/img2320.jpg\nSaved: /kaggle/working/predicted_images/img1115.jpg\nSaved: /kaggle/working/predicted_images/img2036.jpg\nSaved: /kaggle/working/predicted_images/img626.jpg\nSaved: /kaggle/working/predicted_images/img2053.jpg\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"!zip -r /kaggle/working/predicted_images.zip /kaggle/working/predicted_images","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-12T13:10:42.584685Z","iopub.execute_input":"2025-02-12T13:10:42.584985Z","iopub.status.idle":"2025-02-12T13:10:43.122387Z","shell.execute_reply.started":"2025-02-12T13:10:42.584962Z","shell.execute_reply":"2025-02-12T13:10:43.121533Z"}},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/predicted_images/ (stored 0%)\n  adding: kaggle/working/predicted_images/img1116.jpg (deflated 2%)\n  adding: kaggle/working/predicted_images/img587.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img2839.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img2889.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img1736.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img1987.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img1634.jpg (deflated 5%)\n  adding: kaggle/working/predicted_images/img630.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img2040.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img1646.jpg (deflated 3%)\n  adding: kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 15.43.33_b221e73d.jpg (deflated 0%)\n  adding: kaggle/working/predicted_images/img1113.jpg (deflated 1%)\n  adding: kaggle/working/predicted_images/img622.jpg (deflated 13%)\n  adding: kaggle/working/predicted_images/img1637.jpg (deflated 13%)\n  adding: kaggle/working/predicted_images/img1121.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img2837.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img120.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 16.10.30_939018d5.jpg (deflated 0%)\n  adding: kaggle/working/predicted_images/img1745.jpg (deflated 12%)\n  adding: kaggle/working/predicted_images/img2824.jpg (deflated 9%)\n  adding: kaggle/working/predicted_images/img1986.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img1729.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img2895.jpg (deflated 0%)\n  adding: kaggle/working/predicted_images/img2339.jpg (deflated 9%)\n  adding: kaggle/working/predicted_images/img1728.jpg (deflated 10%)\n  adding: kaggle/working/predicted_images/img2333.jpg (deflated 5%)\n  adding: kaggle/working/predicted_images/img118.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img1739.jpg (deflated 3%)\n  adding: kaggle/working/predicted_images/img2340.jpg (deflated 5%)\n  adding: kaggle/working/predicted_images/img922.jpg (deflated 10%)\n  adding: kaggle/working/predicted_images/img2336.jpg (deflated 5%)\n  adding: kaggle/working/predicted_images/img2039.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img1982.jpg (deflated 1%)\n  adding: kaggle/working/predicted_images/img1633.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img583.jpg (deflated 10%)\n  adding: kaggle/working/predicted_images/img2925.jpg (deflated 5%)\n  adding: kaggle/working/predicted_images/img588.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img2316.jpg (deflated 5%)\n  adding: kaggle/working/predicted_images/img111.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img2832.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img114.jpg (deflated 9%)\n  adding: kaggle/working/predicted_images/img2121.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img2823.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img2834.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img2326.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img2830.jpg (deflated 5%)\n  adding: kaggle/working/predicted_images/img2045.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img751.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img1115.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img1635.jpg (deflated 3%)\n  adding: kaggle/working/predicted_images/img2038.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img1647.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img2122.jpg (deflated 5%)\n  adding: kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 16.09.56_60471117.jpg (deflated 1%)\n  adding: kaggle/working/predicted_images/img925.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img754.jpg (deflated 9%)\n  adding: kaggle/working/predicted_images/img1638.jpg (deflated 1%)\n  adding: kaggle/working/predicted_images/img2323.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img2831.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img2835.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img1118.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img1636.jpg (deflated 1%)\n  adding: kaggle/working/predicted_images/img750.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img2049.jpg (deflated 5%)\n  adding: kaggle/working/predicted_images/img2118.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 15.43.32_bfe9e469.jpg (deflated 0%)\n  adding: kaggle/working/predicted_images/img1737.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 16.08.28_0bd91554.jpg (deflated 0%)\n  adding: kaggle/working/predicted_images/img2890.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img2052.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 15.43.34_788c8848.jpg (deflated 0%)\n  adding: kaggle/working/predicted_images/img2037.jpg (deflated 9%)\n  adding: kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 15.43.32_fa8928a5.jpg (deflated 1%)\n  adding: kaggle/working/predicted_images/img2337.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img749.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img103.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img2048.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img920.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img2594.jpg (deflated 11%)\n  adding: kaggle/working/predicted_images/img1644.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img2044.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img112.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img1649.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img2334.jpg (deflated 5%)\n  adding: kaggle/working/predicted_images/img752.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img2120.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/photo-collage.png (deflated 0%)\n  adding: kaggle/working/predicted_images/img1639.jpg (deflated 5%)\n  adding: kaggle/working/predicted_images/img110.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img2897.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img629.jpg (deflated 10%)\n  adding: kaggle/working/predicted_images/img2119.jpg (deflated 5%)\n  adding: kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 16.08.27_8b40b39e.jpg (deflated 0%)\n  adding: kaggle/working/predicted_images/img921.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img107.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img2322.jpg (deflated 3%)\n  adding: kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 16.08.26_d026a843.jpg (deflated 0%)\n  adding: kaggle/working/predicted_images/img584.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img2838.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img2896.jpg (deflated 5%)\n  adding: kaggle/working/predicted_images/img748.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img2321.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img2331.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img2891.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img121.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img2588.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img117.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img1984.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img586.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img2330.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 15.43.41_612db12b.jpg (deflated 0%)\n  adding: kaggle/working/predicted_images/img1632.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img2592.jpg (deflated 5%)\n  adding: kaggle/working/predicted_images/img1740.jpg (deflated 3%)\n  adding: kaggle/working/predicted_images/img2894.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img1733.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img919.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img2590.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img1642.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img1122.jpg (deflated 2%)\n  adding: kaggle/working/predicted_images/img1112.jpg (deflated 5%)\n  adding: kaggle/working/predicted_images/img1730.jpg (deflated 10%)\n  adding: kaggle/working/predicted_images/img2043.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img1983.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img2041.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img106.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img923.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img589.jpg (deflated 5%)\n  adding: kaggle/working/predicted_images/img1743.jpg (deflated 13%)\n  adding: kaggle/working/predicted_images/img2892.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img2591.jpg (deflated 3%)\n  adding: kaggle/working/predicted_images/img2318.jpg (deflated 10%)\n  adding: kaggle/working/predicted_images/img1124.jpg (deflated 1%)\n  adding: kaggle/working/predicted_images/img2123.jpg (deflated 13%)\n  adding: kaggle/working/predicted_images/img1981.jpg (deflated 2%)\n  adding: kaggle/working/predicted_images/img2826.jpg (deflated 5%)\n  adding: kaggle/working/predicted_images/img1111.jpg (deflated 2%)\n  adding: kaggle/working/predicted_images/img2586.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img624.jpg (deflated 13%)\n  adding: kaggle/working/predicted_images/img2820.jpg (deflated 9%)\n  adding: kaggle/working/predicted_images/img2053.jpg (deflated 9%)\n  adding: kaggle/working/predicted_images/img924.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img2124.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img2825.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img2593.jpg (deflated 10%)\n  adding: kaggle/working/predicted_images/img104.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img2317.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img116.jpg (deflated 2%)\n  adding: kaggle/working/predicted_images/img1643.jpg (deflated 2%)\n  adding: kaggle/working/predicted_images/img1731.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img627.jpg (deflated 12%)\n  adding: kaggle/working/predicted_images/img2051.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img1738.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img1126.jpg (deflated 2%)\n  adding: kaggle/working/predicted_images/img105.jpg (deflated 13%)\n  adding: kaggle/working/predicted_images/img2335.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img2046.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img1125.jpg (deflated 2%)\n  adding: kaggle/working/predicted_images/img1120.jpg (deflated 3%)\n  adding: kaggle/working/predicted_images/img1980.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img756.jpg (deflated 9%)\n  adding: kaggle/working/predicted_images/img2325.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img2827.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img2342.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img625.jpg (deflated 5%)\n  adding: kaggle/working/predicted_images/img753.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 16.09.56_80b19fb3.jpg (deflated 0%)\n  adding: kaggle/working/predicted_images/img115.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img2828.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img2319.jpg (deflated 5%)\n  adding: kaggle/working/predicted_images/img1110.jpg (deflated 3%)\n  adding: kaggle/working/predicted_images/img926.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img626.jpg (deflated 11%)\n  adding: kaggle/working/predicted_images/img1742.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img119.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/WhatsApp Image 2024-10-22 at 16.08.26_f2b7c86c.jpg (deflated 1%)\n  adding: kaggle/working/predicted_images/img2047.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img1732.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img755.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img2125.jpg (deflated 3%)\n  adding: kaggle/working/predicted_images/1.jpg (deflated 0%)\n  adding: kaggle/working/predicted_images/img2327.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img2324.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img1985.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img1114.jpg (deflated 1%)\n  adding: kaggle/working/predicted_images/img1641.jpg (deflated 2%)\n  adding: kaggle/working/predicted_images/img109.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img2042.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img2589.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img1123.jpg (deflated 3%)\n  adding: kaggle/working/predicted_images/img927.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img628.jpg (deflated 10%)\n  adding: kaggle/working/predicted_images/img623.jpg (deflated 12%)\n  adding: kaggle/working/predicted_images/img2821.jpg (deflated 9%)\n  adding: kaggle/working/predicted_images/img1119.jpg (deflated 3%)\n  adding: kaggle/working/predicted_images/img2328.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img108.jpg (deflated 3%)\n  adding: kaggle/working/predicted_images/img2332.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img1648.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img1645.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img2036.jpg (deflated 10%)\n  adding: kaggle/working/predicted_images/img1734.jpg (deflated 3%)\n  adding: kaggle/working/predicted_images/img2341.jpg (deflated 5%)\n  adding: kaggle/working/predicted_images/img1741.jpg (deflated 1%)\n  adding: kaggle/working/predicted_images/img2893.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img2338.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img590.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img2818.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img591.jpg (deflated 8%)\n  adding: kaggle/working/predicted_images/img2320.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img1735.jpg (deflated 1%)\n  adding: kaggle/working/predicted_images/img1640.jpg (deflated 5%)\n  adding: kaggle/working/predicted_images/img1127.jpg (deflated 1%)\n  adding: kaggle/working/predicted_images/img2829.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img2329.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img2050.jpg (deflated 6%)\n  adding: kaggle/working/predicted_images/img585.jpg (deflated 7%)\n  adding: kaggle/working/predicted_images/img2117.jpg (deflated 5%)\n  adding: kaggle/working/predicted_images/img1988.jpg (deflated 2%)\n  adding: kaggle/working/predicted_images/img2587.jpg (deflated 5%)\n  adding: kaggle/working/predicted_images/img1744.jpg (deflated 4%)\n  adding: kaggle/working/predicted_images/img1117.jpg (deflated 2%)\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}